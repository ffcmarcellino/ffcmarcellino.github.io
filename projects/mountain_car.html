<!DOCTYPE HTML>
<html>
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Fellipe Marcellino - MountainCar Challenge</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="Solving MountainCar challenge with reinforcement learning" />
      <meta name="keywords" content="portfolio,personal website,github,projects,reinforcement learning,machine learning,gym,openai,mountain car" />
      <meta name="author" content="Fellipe Marcellino" />
      <!--Social Media icons-->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
      <link rel="icon" type="image/png" href="../logo.png">
      <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Montserrat|Open+Sans|Raleway&display=swap" rel="stylesheet">
      <!-- Animate.css -->
      <link rel="stylesheet" href="../css/animate.css">
      <!-- Bootstrap  -->
      <link rel="stylesheet" href="../css/bootstrap.css">
      <!-- Flexslider  -->
      <link rel="stylesheet" href="../css/flexslider.css">
      <!-- Flaticons  -->
      <link rel="stylesheet" href="../fonts/flaticon/font/flaticon.css">
      <!-- Theme style  -->
      <link rel="stylesheet" href="../css/style.css">
      <!-- Modernizr JS -->
      <script src="js/modernizr-2.6.2.min.js"></script>
      <style>
      <!--
       /* Font Definitions */
       @font-face
      	{font-family:Wingdings;
      	panose-1:5 0 0 0 0 0 0 0 0 0;}
      @font-face
      	{font-family:"Cambria Math";
      	panose-1:2 4 5 3 5 4 6 3 2 4;}
      @font-face
      	{font-family:Calibri;
      	panose-1:2 15 5 2 2 2 4 3 2 4;}
      @font-face
      	{font-family:Georgia;
      	panose-1:2 4 5 2 5 4 5 2 3 3;}
       /* Style Definitions */
       p.MsoNormal, li.MsoNormal, div.MsoNormal
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:8.0pt;
      	margin-left:0cm;
      	line-height:107%;
      	font-size:11.0pt;
      	font-family:"Calibri",sans-serif;}
      p.MsoCaption, li.MsoCaption, div.MsoCaption
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:10.0pt;
      	margin-left:0cm;
      	font-size:9.0pt;
      	font-family:"Calibri",sans-serif;
      	color:#44546A;
      	font-style:italic;}
      a:link, span.MsoHyperlink
      	{color:#0563C1;
      	text-decoration:underline;}
      a:visited, span.MsoHyperlinkFollowed
      	{color:#954F72;
      	text-decoration:underline;}
      span.MsoPlaceholderText
      	{color:gray;}
      p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:8.0pt;
      	margin-left:36.0pt;
      	line-height:107%;
      	font-size:11.0pt;
      	font-family:"Calibri",sans-serif;}
      p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:0cm;
      	margin-left:36.0pt;
      	margin-bottom:.0001pt;
      	line-height:107%;
      	font-size:11.0pt;
      	font-family:"Calibri",sans-serif;}
      p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:0cm;
      	margin-left:36.0pt;
      	margin-bottom:.0001pt;
      	line-height:107%;
      	font-size:11.0pt;
      	font-family:"Calibri",sans-serif;}
      p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
      	{margin-top:0cm;
      	margin-right:0cm;
      	margin-bottom:8.0pt;
      	margin-left:36.0pt;
      	line-height:107%;
      	font-size:11.0pt;
      	font-family:"Calibri",sans-serif;}
      .MsoChpDefault
      	{font-family:"Calibri",sans-serif;}
      .MsoPapDefault
      	{margin-bottom:8.0pt;
      	line-height:107%;}
      @page WordSection1
      	{size:595.3pt 841.9pt;
      	margin:70.85pt 70.85pt 70.85pt 70.85pt;}
      div.WordSection1
      	{page:WordSection1;}
       /* List Definitions */
       ol
      	{margin-bottom:0cm;}
      ul
      	{margin-bottom:0cm;}
      -->
      </style>
   </head>
   <body>
      <div id="colorlib-page">
         <div class="container-wrap">
            <a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
            <!----Side bar--->
            <aside id="colorlib-aside" role="complementary" class="border js-fullheight">
               <div class="text-center">
                  <div class="author-img" style="background-image: url(../images/profile.jpg);"></div>
                  <h1 id="colorlib-logo"><a href="https://ffcmarcellino.github.io">Fellipe Marcellino</a></h1>
                  <p><strong>ML enthusiast</strong></p>
               </div>
            </aside>
            <!--- Github ribbon ----------->
            <a href="https://github.com/ffcmarcellino/" class="github-corner" aria-label="View source on GitHub">
               <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
                  <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
                  <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
                  <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
               </svg>
            </a>
            <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
            <!----- main page--------->
            <div id="colorlib-main">
               <section class="colorlib-about">
                  <div class="colorlib-narrow-content">
                     <div class="row">
                        <div class="col-md-12">
                           <div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
                              <h2>MountainCar Challenge</h2>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <hr>
                                    <div class=WordSection1>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>In this environment, we have a car
                                    positioned in a valley between two mountains and we have a yellow flag in the
                                    top right.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>The goal here is to drive the
                                    car up the mountain until it <b>reaches the flag</b> in the <b>least amount of
                                    time</b> possible. The trivial solution would be to accelerate the car only in
                                    the right direction towards the flag. However, there is one caveat: the car's
                                    engine is <b>not powerful enough</b> to go straight up the hill against
                                    gravity. Thus, <b>it needs to learn</b> to rock back and forth to get enough
                                    momentum to reach the goal, as seen in Figure 1:</span></p>

                                    <p class=MsoNormal align=center style='margin-bottom:12.0pt;text-align:center;
                                    page-break-after:avoid'><img width=449 height=298 id="Picture 1"
                                    src="mountain_car_files/image001.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure </span></b><b><span
                                    lang=EN-US style='font-size:11.0pt'>1</span></b><b><span lang=EN-US
                                    style='font-size:11.0pt'>- MountainCar Environment</span></b></p>

                                    <p class=MsoNormal><span lang=EN-US>&nbsp;</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>The open-source code for this
                                    environment can be found </span><span class=MsoHyperlink><span lang=EN-US
                                    style='background:white'><a href="https://gym.openai.com/envs/MountainCar-v0/">here</a></span></span><span
                                    lang=EN-US style='color:black;background:white'>.</span></p>

                                    <p class=MsoNormal style='margin-top:18.0pt;margin-right:0cm;margin-bottom:
                                    12.0pt;margin-left:0cm;line-height:150%'><b><span lang=EN-US style='font-size:
                                    14.0pt;line-height:150%;font-family:"Georgia",serif'>How to solve this problem?</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>This is an optimal control
                                    problem, that is, we are interested in finding an <b>optimal policy</b> to
                                    control the vehicle's engine in order to optimize an objective function. In
                                    this case, the objective function is related to the total time for the vehicle
                                    to reach the flag.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>If we knew the dynamics of the
                                    system, this problem could be solved using exact solutions well-known in the
                                    literature, such as <b>Dynamic Programming</b> algorithms like the <b>Linear
                                    Quadratic Regulator</b>. However, we assume that we don't have the equations
                                    that rule the changes in the system, which is a reasonable assumption. For
                                    instance, in the real-world this system is affected by friction between the car
                                    and the environment. Although we may have prior knowledge of the coefficients
                                    of friction in some cases, in others we must find it empirically.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>From our assumption that we
                                    don't know the dynamics of the system, we must learn the optimal control from <b>experience</b>.
                                    In order to do that, we can rely on <b>Reinforcement Learning</b> methods,
                                    which are described in the following sections.</span></p>

                                    <p class=MsoNormal style='margin-top:18.0pt;margin-right:0cm;margin-bottom:
                                    12.0pt;margin-left:0cm;line-height:150%'><b><span lang=EN-US style='font-size:
                                    14.0pt;line-height:150%;font-family:"Georgia",serif'>Defining the elements of
                                    the RL system</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>A Reinforcement Learning system
                                    contains several elements that need to be defined and are related to each other
                                    as shown in Figure 2:</span></p>

                                    <p class=MsoCaption align=center style='margin-bottom:0cm;margin-bottom:.0001pt;
                                    text-align:center;page-break-after:avoid'><img border=0 width=605 height=289
                                    id="Picture 2" src="mountain_car_files/image002.png"
                                    alt="https://lh4.googleusercontent.com/x2CzIkDus_hbDmUtWpS-EyVVZANuhNoQNv65bPLOYEEXYOxZuXR8NQVmdrrXVxebLRyH_NP5NeZKwLOH_fDGGJN2SycSVSc6CpL-wDwJ_aoQP8T7ncdMhK4oHVK6nMcqPP43vBAxOzo"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure </span></b><b><span
                                    lang=EN-US style='font-size:11.0pt'>2</span></b><b><span lang=EN-US
                                    style='font-size:11.0pt'> - Reinforcement Learning Elements [1]</span></b></p>

                                    <p class=MsoCaption align=center style='margin-bottom:0cm;margin-bottom:.0001pt;
                                    text-align:center'><span lang=EN-US>'</span></p>

                                    <p class=MsoListParagraphCxSpFirst style='text-indent:-18.0pt;line-height:150%'><i><span
                                    lang=EN-US style='color:black'>1.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span></i><i><u><span lang=EN-US style='color:black;background:white'>Agent</span></u></i></p>

                                    <p class=MsoListParagraphCxSpMiddle style='line-height:150%'><span lang=EN-US
                                    style='color:black;background:white'>The agent of the system is the
                                    decision-maker. In this case, the <b>driver</b> of the vehicle is the agent.</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='text-indent:-18.0pt;line-height:
                                    150%'><i><span lang=EN-US style='color:black'>2.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span></i><i><u><span lang=EN-US style='color:black;background:white'>Environment</span></u></i></p>

                                    <p class=MsoListParagraphCxSpMiddle style='line-height:150%'><span lang=EN-US
                                    style='color:black;background:white'>As mentioned above, the environment is
                                    comprised by the <b>mountains</b> and the <b>flag</b>, with which the vehicle
                                    interacts.</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='text-indent:-18.0pt;line-height:
                                    150%'><i><span lang=EN-US style='color:black'>3.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span></i><i><u><span lang=EN-US style='color:black;background:white'>Actions</span></u></i></p>

                                    <p class=MsoListParagraphCxSpMiddle style='line-height:150%'><span lang=EN-US
                                    style='color:black;background:white'>The set of possible actions that the
                                    driver can make is discrete:</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='margin-left:54.0pt;text-indent:-18.0pt;
                                    line-height:150%'><span lang=EN-US style='color:black'>-<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span><b><span lang=EN-US style='color:black;background:white'>PUSH
                                    LEFT</span></b><span lang=EN-US style='color:black;background:white'>: 0</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='margin-left:54.0pt;text-indent:-18.0pt;
                                    line-height:150%'><span lang=EN-US style='color:black'>-<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span><b><span lang=EN-US style='color:black;background:white'>NO PUSH</span></b><span
                                    lang=EN-US style='color:black;background:white'>: 1</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='margin-left:54.0pt;text-indent:-18.0pt;
                                    line-height:150%'><span lang=EN-US style='color:black'>-<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span><b><span lang=EN-US style='color:black;background:white'>PUSH
                                    RIGHT</span></b><span lang=EN-US style='color:black;background:white'>: 2</span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='text-indent:-18.0pt;line-height:
                                    150%'><i><span lang=EN-US style='color:black'>4.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span></i><i><u><span lang=EN-US style='color:black;background:white'>State</span></u></i></p>

                                    <p class=MsoListParagraphCxSpMiddle style='line-height:150%'><span lang=EN-US
                                    style='color:black;background:white'>The state is a 2-D vector containing the <b>current</b>
                                    <b>position</b> of the vehicle (from -1.2 to 0.5) and the <b>current</b> <b>velocity</b>
                                    (from -0.07 to 0.07) in the form <b>s = (position, velocity)</b></span></p>

                                    <p class=MsoListParagraphCxSpMiddle style='text-indent:-18.0pt;line-height:
                                    150%'><i><span lang=EN-US style='color:black'>5.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                    </span></span></i><i><u><span lang=EN-US style='color:black;background:white'>Reward</span></u></i></p>

                                    <p class=MsoListParagraphCxSpLast style='line-height:150%'><span lang=EN-US
                                    style='color:black;background:white'>Finally, the reward is <b>-1</b> for every
                                    time step.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>With this reward function, we
                                    are incentivizing the agent to reach the terminal state as fast as possible,
                                    and that is indeed what we want.</span></p>

                                    <p class=MsoNormal style='margin-top:18.0pt;margin-right:0cm;margin-bottom:
                                    12.0pt;margin-left:0cm;line-height:150%'><b><span lang=EN-US style='font-size:
                                    14.0pt;line-height:150%;font-family:"Georgia",serif'>State Aggregation: from
                                    continuous to discrete</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='font-size:10.5pt;line-height:150%;color:black;background:
                                    white'>As mentioned in the previous section, the state can be characterized by
                                    a vector of the form s = (position, velocity), where the position and velocity
                                    of the vehicle are two bounded scalars belonging to </span><span lang=EN-US
                                    style='font-family:"Cambria Math",serif;color:#222222;background:white'>&#8477;.
                                    </span><span lang=EN-US style='color:#222222;background:white'>Thus, they are <b>continuous</b>
                                    values.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:#222222;background:white'>However, in order to apply
                                    tabular methods of reinforcement learning, we need to work with a <b>discrete</b>
                                    state space. That's when <b>State Aggregation</b> comes to place.</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;line-height:150%;
                                    page-break-after:avoid'><img border=0 width=439 height=314 id="Picture 6"
                                    src="mountain_car_files/image003.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure </span></b><b><span
                                    lang=EN-US style='font-size:11.0pt'>3</span></b><b><span lang=EN-US
                                    style='font-size:11.0pt'> - Gridline division of state space</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:#222222;background:white'>This technique consists of
                                    dividing the continuous state space into subgroups, where each subgroup
                                    contains a high-dimensional interval of the state space. All the states in each
                                    subgroup will share the same properties, such as reward distribution and state
                                    value. Actually, State Aggregation can be considered a special case of linear
                                    function approximation, where the subgroups are features of the state space.
                                    This, however, is out of the scope of this project.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:#222222;background:white'>One way to divide the state
                                    space is with the <b>gridline</b> <b>method</b>, by setting parallel lines in
                                    each dimension with constant intervals, as shown in Figure 3:</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:#222222;background:white'>Using this method, I'll
                                    subdivide the <b>position</b> state variable into <b>17 intervals of length 0.1</b>
                                    and the <b>velocity</b> state variable into <b>14 intervals of length 0.01</b>.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:#222222;background:white'>Therefore, I'll convert a
                                    continuous 2-dimensional state space into a discrete space of <b>17*14 = 238
                                    states</b>.</span></p>

                                    <p class=MsoNormal style='line-height:150%'><b><span lang=EN-US
                                    style='font-size:14.0pt;line-height:150%;font-family:"Georgia",serif'>Tabular
                                    Q-learning</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>The <b>tabular Q-learning</b>
                                    algorithm is based on <b>updates</b> of the state-action value function <b>Q(s,a)</b>
                                    for each state and action in the discrete space. Usually, we store those
                                    Q-values in a table, with each line corresponding to a state and each column to
                                    an action. Hence the name 'tabular' Q-learning.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>The update rule for the
                                    Q-values follows the '<b>Bellman-equivalent</b>' equation:</span></p>

                                    <p class=MsoNormal style='text-align:justify;text-indent:35.4pt;line-height:
                                    150%'><b><span lang=EN-US style='color:black;background:white'>Q(S<sub>t</sub>,
                                    A<sub>t</sub>) = </span></b><b><span lang=EN-US style='color:
                                    black;background:white'> Q(S<sub>t</sub>, A<sub>t</sub>) + &#945;[R<sub>t+1 </sub>+
                                    &#978;max<sub>a</sub>Q(S<sub>t+1</sub>, a) - Q(S<sub>t</sub>, A<sub>t</sub>),</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>where <b>t</b> corresponds to
                                    the time step, <b>S</b> is the state, <b>A</b> is the action, <b>&#945;</b> is
                                    the step size and <b>&#978;</b> is the discount factor.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='color:black;background:white'>The complete algorithm can be
                                    seen in Figure 4 below:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;line-height:150%;
                                    page-break-after:avoid'><img border=0 width=518 height=217 id="Picture 3"
                                    src="mountain_car_files/image004.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 4- Tabular Q-learning algorithm [2]</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>For this problem, I used the </span><b><span lang=EN-US>&#1013;-greedy
                                    policy</span></b><span lang=EN-US> as the behavior policy and</span><span
                                    lang=EN-US> I trained the agent with a grid of hyper parameters as follows:</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
                                    lang=EN-US style='color:black;background:white'>&#945; in {0.5, 0.1, 0.01}</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
                                    lang=EN-US>&#1013; in {0.5, 0.01}</span></b><span lang=EN-US>, where &#1013; is
                                    the exploration factor for the &#1013;-greedy policy</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><b><span
                                    lang=EN-US style='color:black;background:white'>&#978; = 1</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>As the Mountain Car control is an <b>episodic</b> <b>task</b> (an
                                    episode ends when the car reaches the flag), we don't need to set a discount
                                    factor less than 1. </span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Each combination of hyper parameters was then applied to an agent
                                    for <b>10 runs</b> of <b>500 episodes </b>each. The number of steps it took the
                                    agent to finish the episode was averaged over the 10 runs for each episode and
                                    then averaged again over 50-episode ranges. The results can be seen in Figure
                                    5:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=516 height=383 id="Picture 10"
                                    src="mountain_car_files/image005.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 5 - Learning behavior for different hyper
                                    parameter sets</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>We can see from the plot above that <b>large step sizes converge
                                    faster</b>, but with <b>worse performance</b>. Ideally, we would want to train
                                    agents over a large number of episodes with a small step size. However, the
                                    incremental performance we get is not worth the increase in time. Thus, a <b>step
                                    size of 0.1</b> seems reasonable.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Narrowing our agents to <b>[&#1013; = 0.5, <span style='color:black;
                                    background:white'>&#945; = 0.1]</span></b><span style='color:black;background:
                                    white'> and </span><b>[&#1013; = 0.1, <span style='color:black;background:white'>&#945;
                                    = 0.1]</span></b><span style='color:black;background:white'>, we can train them
                                    again this time with <b>1 run of 2,000 episodes</b> instead of 500. Figure 6
                                    shows the results:</span></span></p>

                                    <p class=MsoNormal align=center style='text-align:center'><img border=0
                                    width=446 height=327 id="Picture 17" src="mountain_car_files/image006.jpg"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 6 - Learning behavior for chosen hyper
                                    parameters</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>We can observe from Figure 5 and 6 that <b>larger epsilon values
                                    converge with worse performance</b> than small values. This is expected, as a
                                    higher epsilon implies that the agent will spend more time exploring actions
                                    that are not optimal, which impacts negatively the number of steps to finish an
                                    episode. Thus, we <b>can't use this plot</b> to compare the performance of
                                    different values of epsilon on the final policy, as the final policy will be
                                    greedy and not &#1013;-greedy.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Instead, we can apply the <b>final policy</b> learned by the agent
                                    over <b>random initial states</b> and plot the distribution of number of steps
                                    taken to finish an episode, as seen in Figure 7 and Figure 8:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=403 height=269 id="Picture 19"
                                    src="mountain_car_files/image007.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 7 - Number of steps to finish episode vs
                                    initial position</span></b></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=383 height=254 id="Picture 18"
                                    src="mountain_car_files/image008.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 8 - Boxplot of number of steps to finish
                                    episode for each epsilon</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>We can see that, although the exploration parameter &#1013; is very
                                    different, both agents performed similarly. This is due to the '<b>optimistic
                                    initialization</b>' of the Q function by the agent. An optimistic
                                    initialization, as the name suggests, sets optimistic starting values for
                                    Q(s,a). Thus, when the algorithm explores states and actions and updates the
                                    corresponding Q(s,a), the Q-values will decrease. Then, the &#1013;</span><span
                                    lang=EN-US>-greedy policy will prioritize <b>unexplored</b> states, even for
                                    small values of </span><span lang=EN-US>&#1013;.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>According to Figure 7, <b>&#1013; = 0.5 </b>performed better when
                                    the vehicle started in the right part of the mountain and, overall, it had a
                                    better average number of steps. Then, it would make sense to conclude that <b>&#1013;
                                    = 0.5 </b>is better than <b>&#1013; = 0.1</b>. However, the final policy of the
                                    agents has a high variance. For example, if we train both agents again with a
                                    different random seed, we obtain a new plot (Figure 9):</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=649 height=229 id="Group 22"
                                    src="mountain_car_files/image009.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 9 - Number of steps to finish episode for
                                    random initial states</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US style='font-size:10.5pt;line-height:150%;color:black;background:
                                    white'>This time, </span><b><span lang=EN-US>&#1013; = 0.1 </span></b><span
                                    lang=EN-US>seems a better option. Thus, the <b>variance is too high</b> to
                                    reach any conclusion. One possible explanation is that the Q value function is <b>not
                                    generalizing</b> along states. This problem will be analyzed in this article
                                    afterwards.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>If we choose <b>&#1013; = 0.1 </b>and<b> </b></span><b><span
                                    lang=EN-US style='font-size:10.5pt;line-height:150%;color:black;background:
                                    white'>&#945; = 0.1</span></b><span lang=EN-US style='font-size:10.5pt;
                                    line-height:150%;color:black;background:white'> and train the agent after 3,000
                                    episodes, it was able to learn how to reach the flag in less than 200 steps,
                                    regardless of the initial position.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>In the first episodes, the algorithm was exploring new states and
                                    actions and thus the vehicle moved <b>erratically back and forth</b>, as seen in
                                    Figure 10:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=480 height=321 id="Picture 8"
                                    src="mountain_car_files/image010.gif"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 10 - Demo of control after 19 episodes of
                                    training</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>After around 200 episodes, the algorithm started converging to an
                                    optimal policy. A demo of the final control can be seen in Figure 11:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;page-break-after:avoid'><img
                                    border=0 width=478 height=319 id="Picture 5"
                                    src="mountain_car_files/image011.gif"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 11 - Demo of control after 3000 episodes of
                                    training</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Figure 12 shows an illustration in 2 dimensions of the final policy
                                    found by the agent:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;line-height:150%;
                                    page-break-after:avoid'><img border=0 width=443 height=267 id="Picture 24"
                                    src="mountain_car_files/image012.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 12 - Final policy representation</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>We can see that for negative velocities the most common action is is
                                    to push left and for positive velocities is to push right, which makes sense if
                                    we want to build momentum. However, there are some inconsistencies in the
                                    policy, such as the top right corner. If the velocity is positive and high and
                                    the vehicle is close to the flag, pushing left will only delay the end of the
                                    episode. Again, this may be due to a lack of generalization.</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Indeed, if we look at the number of visits for each state-action
                                    pair, a lot of them were not visited by the agent, mainly the positions around
                                    the edge of the grid, as shown in Figure 13:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;line-height:150%;
                                    page-break-after:avoid'><img border=0 width=378 height=839 id="Group 28"
                                    src="mountain_car_files/image013.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 13 - State-action visits while training the
                                    agent</span></b></p>

                                    <p class=MsoNormal style='line-height:150%'><span lang=EN-US>&nbsp;</span></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>Finally, the value function for the final policy can be seen in
                                    Figure 14 below:</span></p>

                                    <p class=MsoNormal align=center style='text-align:center;line-height:150%;
                                    page-break-after:avoid'><img border=0 width=398 height=270 id="Picture 29"
                                    src="mountain_car_files/image014.png"></p>

                                    <p class=MsoCaption align=center style='text-align:center'><b><span lang=EN-US
                                    style='font-size:11.0pt'>Figure 14 - Value function for the final policy</span></b></p>

                                    <p class=MsoNormal style='text-align:justify;line-height:150%'><span
                                    lang=EN-US>The minimum value can be found when the vehicle is in the bottom of
                                    the mountain with null velocity, which makes sense, as this is the state with
                                    the lowest energy. As the vehicle gets further from this state, the value
                                    increases in a spiral shape around it.</span></p>

                                    <br><br>
                                    <p class=MsoNormal style='line-height:150%'>[1] <span class=MsoHyperlink><a
                                    href="https://images.app.goo.gl/CKnvseNTxRkppGki9">https://images.app.goo.gl/CKnvseNTxRkppGki9</a></span></p>

                                    <p class=MsoNormal style='line-height:150%'><span lang=EN-US>[2] </span><span
                                    lang=EN-US style='color:#222222;background:white'>Sutton, R. S., Barto, A. G.
                                    Reinforcement Learning: An Introduction. Cambridge, MA: The MIT ''Press, 2018,
                                    Second edition, p. 131</span></p>

                                    </div>
                                 </div>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
               </section>
               <!--------Contact---------->
               <section class="colorlib-footer" data-section="footer">
                  <div class="colorlib-narrow-content">
                     <div class="row">
                        <div class="col-md-12">
                           <div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
                              <h2>Contact</h2>
                              <hr>
                              <div class="hire">
                                 <div align=center>
                                    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
                                    <a href="http://github.com/ffcmarcellino"><i class="fa fa-github" style="font-size:24px; position:relative; left:-30px; top: -10px; color:black"></i></a>
                                    <a href="http://linkedin.com/in/fellipemarcellino"><i class="fa fa-linkedin" style="font-size:24px; position:relative; left:30px; top: -10px; color: black"></i></a>
                                    <p style="text-align: center; margin-bottom: -20px"><strong>&copy;2019 Fellipe Marcellino</strong></p>
                                    <br>
                                 </div>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
               </section>
            </div>
            <!-- end:colorlib-main -->
         </div>
         <!-- end:container-wrap -->
      </div>
      <!-- end:colorlib-page -->
      <!-- jQuery -->
      <script src="js/jquery.min.js"></script>
      <!-- jQuery Easing -->
      <script src="js/jquery.easing.1.3.js"></script>
      <!-- Bootstrap -->
      <script src="js/bootstrap.min.js"></script>
      <!-- Waypoints -->
      <script src="js/jquery.waypoints.min.js"></script>
      <!-- Flexslider -->
      <script src="js/jquery.flexslider-min.js"></script>
      <!-- Counters -->
      <script src="js/jquery.countTo.js"></script>
      <!-- MAIN JS -->
      <script src="js/main.js"></script>
   </body>
</html>
