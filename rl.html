<!DOCTYPE HTML>
<html>
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Fellipe Marcellino - RL Projects</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="Reinforcement Learning projects of Fellipe Marcellino" />
      <meta name="keywords" content="portfolio,personal website,github,projects,reinforcement learning,machine learning" />
      <meta name="author" content="Fellipe Marcellino" />
      <!--Social Media icons-->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
      <link rel="icon" type="image/png" href="logo.png">
      <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Montserrat|Open+Sans|Raleway&display=swap" rel="stylesheet">
      <!-- Animate.css -->
      <link rel="stylesheet" href="css/animate.css">
      <!-- Bootstrap  -->
      <link rel="stylesheet" href="css/bootstrap.css">
      <!-- Flexslider  -->
      <link rel="stylesheet" href="css/flexslider.css">
      <!-- Flaticons  -->
      <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
      <!-- Theme style  -->
      <link rel="stylesheet" href="css/style.css">
      <!-- Modernizr JS -->
      <script src="js/modernizr-2.6.2.min.js"></script>
   </head>
   <body>
      <div id="colorlib-page">
         <div class="container-wrap">
            <a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
            <!----Side bar--->
            <aside id="colorlib-aside" role="complementary" class="border js-fullheight">
               <div class="text-center">
                  <div class="author-img" style="background-image: url(images/profile.jpg);"></div>
                  <h1 id="colorlib-logo"><a href="https://ffcmarcellino.github.io">Fellipe Marcellino</a></h1>
                  <p><strong>ML enthusiast</strong></p>
               </div>
            </aside>
            <!--- Github ribbon ----------->
            <a href="https://github.com/ffcmarcellino/" class="github-corner" aria-label="View source on GitHub">
               <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
                  <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
                  <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
                  <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
               </svg>
            </a>
            <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
            <!----- main page--------->
            <div id="colorlib-main">
               <section class="colorlib-about">
                  <div class="colorlib-narrow-content">
                     <div class="row">
                        <div class="col-md-12">
                           <div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
                              <h2>Reinforcement Learning Projects</h2>
                              <hr>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/@fellipe_marcellino/multi-armed-bandits-comparing-different-algorithms-c5925eed87b" target="_blank"><h3>Multi-armed Bandits</h3></a>
                                    <a href="https://medium.com/@fellipe_marcellino/multi-armed-bandits-comparing-different-algorithms-c5925eed87b" target="_blank"><img src="images/kbandits.png" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/traffic_time_bandits">Code</a>&nbsp;&nbsp;<a href="https://medium.com/@fellipe_marcellino/multi-armed-bandits-comparing-different-algorithms-c5925eed87b">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I give a brief introduction on the Multi-armed Bandits problem, I experiment with multiple approaches to solve it and I compare the results. I also introduce the Contextual Bandits problem.</p>
                                 <p align="justify">The environment used to train the agents has 10 actions representing routes to go from work to home and it returns as reward the time spent in traffic in that route. The goal of the agents is to find the fastest route (in expectation).</p>
                                 <p><i>Created on: April 10, 2023&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: April 10, 2023</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/@fellipe_marcellino/solving-a-maze-using-dynamic-programming-5d7272d393" target="_blank"><h3>Generalized Policy Iteration</h3></a>
                                    <a href="https://medium.com/@fellipe_marcellino/solving-a-maze-using-dynamic-programming-5d7272d393" target="_blank"><img src="images/maze.png" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/maze_gpi">Code</a>&nbsp;&nbsp;<a href="https://medium.com/@fellipe_marcellino/solving-a-maze-using-dynamic-programming-5d7272d393">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I introduce a few concepts that are the foundations to more advanced RL algorithms, such as an MDP, value functions and Bellman Equations. Then, I solve the maze problem using dynamic programming: Policy Iteration, Value Iteration and Generalized Policy Iteration.</p>
                                 <p align="justify">The environment used to train the agents was taken from this <a href="https://github.com/MattChanTK/gym-maze" target="_blank">github repository</a> and a few adaptations were made.</p>
                                 <p><i>Created on: June 12, 2023&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: June 12, 2023</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/@fellipe_marcellino/monte-carlo-rl-learning-to-play-tic-tac-toe-b2085aef5bdb" target="_blank"><h3>Monte Carlo Tic Tac Toe</h3></a>
                                    <a href="https://medium.com/@fellipe_marcellino/monte-carlo-rl-learning-to-play-tic-tac-toe-b2085aef5bdb" target="_blank"><img src="images/tictactoe.jpg" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/tictactoe_montecarlo">Code</a>&nbsp;&nbsp;<a href="https://medium.com/@fellipe_marcellino/monte-carlo-rl-learning-to-play-tic-tac-toe-b2085aef5bdb">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I introduce Monte Carlo methods for estimating value functions, as well as different strategies of exploration of the state-action space. I also explain the concept of an after-state and how it is useful. Finally, I train an agent to learn how to play Tic Tac Toe against a random opponent and an advanced one.</p>
                                 <p align="justify">The Tic Tac Toe environment used to train the agents was built from scratch and has the options of a random or advanced opponent to play with.</p>
                                 <p><i>Created on: August 14, 2023&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: August 14, 2023</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/@fellipe_marcellino/off-policy-reinforcement-learning-with-monte-carlo-302807be32db" target="_blank"><h3>Off-Policy Monte Carlo</h3></a>
                                    <a href="https://medium.com/@fellipe_marcellino/off-policy-reinforcement-learning-with-monte-carlo-302807be32db" target="_blank"><img src="images/minigrid.jpg" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/mc_offpolicy">Code</a>&nbsp;&nbsp;<a href="https://medium.com/@fellipe_marcellino/off-policy-reinforcement-learning-with-monte-carlo-302807be32db">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I extend the Monte Carlo algorithms to the off-policy setting and I introduce different methodologies, such as Ordinary and Weighted Importance Sampling, Discount-Aware Importance Sampling and Per-Decision Importance Sampling.</p>
                                 <p align="justify">The environment used to train the agents is a customized version of the <a href="https://minigrid.farama.org/environments/minigrid/">Mini-grid environment</a> from the Farama Foundation.</p>
                                 <p><i>Created on: September 29, 2023&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: September 29, 2023</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/@fellipe_marcellino/temporal-difference-learning-from-sarsa-to-q-learning-671994c80b2e" target="_blank"><h3>Temporal Difference Learning</h3></a>
                                    <a href="https://medium.com/@fellipe_marcellino/temporal-difference-learning-from-sarsa-to-q-learning-671994c80b2e" target="_blank"><img src="images/minigrid_multigoal.png" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/minigrid_temp_diff">Code</a>&nbsp;&nbsp;<a href="https://medium.com/@fellipe_marcellino/temporal-difference-learning-from-sarsa-to-q-learning-671994c80b2e">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I talk about Temporal Difference methods to learn an optimal policy. More specifically, I compare different algorithms of the class of TD(0) methods, such as Sarsa, Expected Sarsa, Q-Learning and Double Q-Learning.</p>
                                 <p align="justify">The environment used to train the agents is a customized version of the <a href="https://minigrid.farama.org/environments/minigrid/">Mini-grid environment</a> from the Farama Foundation.</p>
                                 <p><i>Created on: March 12, 2024&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: March 12, 2024</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="https://medium.com/p/4c6b314f21cc" target="_blank"><h3>n-Step TD Learning</h3></a>
                                    <a href="https://medium.com/p/4c6b314f21cc" target="_blank"><img src="images/taxi.png" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <a href="https://github.com/ffcmarcellino/applied-reinforcement-learning/tree/main/projects/taxi_nstep">Code</a>&nbsp;&nbsp;<a href="https://medium.com/p/4c6b314f21cc">Article</a>
                                 <br><br>
                                 <p align="justify">In this project, I unify Monte Carlo and TD Learning by introducing n-Step TD Learning.</p>
                                 <p align="justify">I compare this algorithm on multiple values of n, both on and off-policy, using the <a href="https://gymnasium.farama.org/environments/toy_text/taxi">Taxi environment</a> from the Farama Foundation to train the agents.</p>
                                 <p><i>Created on: August 29, 2024&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: August 29, 2024</i></p>
                                 <hr>
                              </div>
                              <div class="col-md-12">
                                 <div class="about-desc">
                                    <a href="projects/mountain_car.html" target="_blank"><h3>MountainCar Challenge</h3></a>
                                    <a href="projects/mountain_car.html" target="_blank"><img src="images/mountaincar.png" style="border:3px solid black"></img></a>
                                 </div>
                                 <a href="">Code</a>&nbsp;&nbsp;<a href="projects/mountain_car.html">Article</a>
                                 <br><br>
                                 <p align="justify">The goal of this challenge is to drive the car up the mountain on the right and pass the yellow flag. However, the car's engine is not strong enough to reach the top of the hill by itself. Therefore, the only way to succeed is to drive back and forth to build up momentum.</p>
                                 <p align="justify">In this project, I applied reinforcement learning algorithms to find an optimal policy for the car to control its engine power in order to reach the yellow flag the fastest way possible. I start by using tabular Q-learning with state aggregation, followed by linear function approximation methods.</p>
                                 <p align="justify">The environment with the mountain and car can be found at the <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car/" target="_blank">Gymnasium website</a>.</p>
                                 <p><i>Created on: April 11, 2020&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: April 11, 2020</i></p>
                              </div>
                              <!-- <div class="col-md-12">
                                 <div class="about-desc">
                                    <hr>
                                    <a href="projects/otm.html" target="_blank"><h3>Traffic Light Control Optimization</h3></a>
                                    <a href="projects/otm.html" target="_blank"><img src="images/otm.jpg" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                                 <br><br>
                                 <p align="justify">Vehicle congestion is one the biggest problems in large cities and metropolitan areas. The opportunity cost of being stuck in traffic has a large impact in the economy, in addition to problems such as air pollution and road accidents led by stress.</p>
                                 <p align="justify">One of the causes of traffic jams is poor traffic light timing and synchronization. Current systems usually have fixed policies that become outdated and don't adapt to changes in the environment.</p>
                                 <p align="justify">In this project, I use reinforcement learning techniques (Q-learning with linear function approximation) to find near-optimal policies to control traffic lights in small networks of 2 and 3 intersections. The goal is to minimize the length of the queues of vehicles stopped in a red light. The environment dynamics is ruled by OTM, an open-source traffic model developed at UC Berkeley.</p>
                                 <p><i>Created on: April 21, 2020&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Last modified: April 21, 2020</i></p>
                              </div> -->
                              <!-- <div class="col-md-12">
                                 <div class="about-desc">
                                    <hr>
                                    <a href="projects/terminal_game.html" target="_blank"><h3>_Terminal Game Strategy</h3></a>
                                    <a href="projects/terminal_game.html" target="_blank"><img src="images/terminalgame.jpg" style="border:3px solid black;width:449px;height:298px"></img></a>
                                 </div>
                              </div> -->
                           </div>
                        </div>
                     </div>
                  </div>
               </section>
               <!--------Contact---------->
               <section class="colorlib-footer" data-section="footer">
                  <div class="colorlib-narrow-content">
                     <div class="row">
                        <div class="col-md-12">
                           <div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
                              <h2>Contact</h2>
                              <hr>
                              <div class="hire">
                                 <div align=center>
                                    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
                                    <a href="http://github.com/ffcmarcellino"><i class="fa fa-github" style="font-size:24px; position:relative; left:-30px; top: -10px; color:black"></i></a>
                                    <a href="http://linkedin.com/in/fellipemarcellino"><i class="fa fa-linkedin" style="font-size:24px; position:relative; left:30px; top: -10px; color: black"></i></a>
                                    <p style="text-align: center; margin-bottom: -20px"><strong>&copy;2019 Fellipe Marcellino</strong></p>
                                    <br>
                                 </div>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
               </section>
            </div>
            <!-- end:colorlib-main -->
         </div>
         <!-- end:container-wrap -->
      </div>
      <!-- end:colorlib-page -->
      <!-- jQuery -->
      <script src="js/jquery.min.js"></script>
      <!-- jQuery Easing -->
      <script src="js/jquery.easing.1.3.js"></script>
      <!-- Bootstrap -->
      <script src="js/bootstrap.min.js"></script>
      <!-- Waypoints -->
      <script src="js/jquery.waypoints.min.js"></script>
      <!-- Flexslider -->
      <script src="js/jquery.flexslider-min.js"></script>
      <!-- Counters -->
      <script src="js/jquery.countTo.js"></script>
      <!-- MAIN JS -->
      <script src="js/main.js"></script>
   </body>
</html>
